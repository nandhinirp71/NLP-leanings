Basic Teminologies Used in NLP

1. Corpus
Group of available text data points or it is also refereed to a paragraph of text.

2. Documents
Individual sentence in the data set.

3. Vocabulary
Total number of Unique words in the dataset.

4. Words
Individual words in the dataset.


Techniques for converting words to vectors:

One Hot Encoding
One-Hot Encoding is a technique used in machine learning and Natural Language Processing (NLP) to convert categorical variables (non-numeric data) into a numerical format that can be used in algorithms. It transforms each category into a binary vector, where:

* 1 represents the presence of a specific category, and
* 0 represents the absence of other categories.

How One-Hot Encoding Works:
Consider a categorical variable color with the following categories: ["red", "blue", "green"].

Using one-hot encoding, this variable can be represented as:

Color	   Red	 Blue	  Green
----------------------------
Red	     1	   0	    0
Blue	   0	   1	    0
Green	   0	   0	    1

For each unique category, a new binary column is created. In this case:

* "Red" is represented as [1, 0, 0],
* "Blue" as [0, 1, 0],
* "Green" as [0, 0, 1].

Advantages of OHE:
i. Simple to Implement
ii. Intuitive (Easy to Understand)

Disadvantages of OHE:
i. Creates Sparse Matrix (matrix with more zeroes), and sparse matrix consumes lot of memory and computation time
ii. Out of Vocabulary (OOV), words present in test data is not present in train data
iii. sentences in the dataset is not of fixed length
iv. Semantic meaning between words are not captured, not able to find relationship between words


Bag of Words (Words are converted to features)

The Bag of Words (BoW) model is a popular technique in Natural Language Processing (NLP) used to represent text data in a numerical format that can be processed by machine learning algorithms. It simplifies text by treating it as a "bag" (collection) of words, disregarding grammar, word order, or context but focusing on word occurrence or frequency.

Example:
Assume you have the following two sentences (documents):

Sentence 1: "I love playing football"
Sentence 2: "I love watching football"

Step 1: Tokenization
Break the sentences into individual words:

Sentence 1: ["I", "love", "playing", "football"]
Sentence 2: ["I", "love", "watching", "football"]

Step 2: Vocabulary Creation
The unique words from both sentences form the vocabulary: ["I", "love", "playing", "watching", "football"]

Step 3: Word Representation
Now represent each sentence as a vector based on the word occurrences in the vocabulary:

            I	  love	playing	  watching	football
---------------------------------------------------
Sentence1	  1	  1	    1	        0	        1
Sentence2	  1	  1	    0	        1	        1

Here:
Each sentence is a vector with 5 positions (one for each word in the vocabulary).
A value of 1 indicates the word is present in the sentence, and 0 indicates it is absent.
Alternatively, the values could be the frequency of the word (if a word appears multiple times in the same sentence).

NOTE: 
Binary BOW - 1 representes presence of a word, 0 represents absence of a word

Advantages of BOW:
i. It is simple and intuitive

Disadvantages of BOW:
i. Presence of Sparsity in case of large dataset
ii. OOV 
iii. Ordering of words is changed completely
iv. Semantic meaning is not captured (Consine similarity is used to capture sematic meanings)

NOTE: 
N-grams
* N-grams is used to capture semantic meaning of corpus


